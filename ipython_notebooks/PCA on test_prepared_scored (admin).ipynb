{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.10"
    },
    "analyzedDataset": "test_prepared_scored",
    "creator": "admin",
    "tags": [],
    "customFields": {}
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Principal Component Analysis (PCA) on test_prepared_scored"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The goal of Principal Component Analysis (PCA) is to reduce the **number of dimensions** of a d-dimensional dataset by projecting it onto a k-dimensional subspace (with k \u003c d) in order to increase the **computational efficiency** while retaining most of the information.\n",
        "\n",
        "The k dimensions that we keep (eigenvectors) are called \"**principal components**\".\n",
        "\n",
        "The PCA approach requires to:\n",
        "\n",
        "* Standardize the data.\n",
        "* Obtain the Eigenvectors and Eigenvalues from a Singular Vector Decomposition (SVD).\n",
        "* Choose the number k of principal components to keep.\n",
        "* Construct a projection matrix with the selected k eigenvectors.\n",
        "* Project original dataset to a k-dimensional feature subspace.\n",
        "\n",
        "Choosing the number k can be done systematically by selecting the components that best describe the variance in our data. The amount of information (variance) contained by each eigenvector can be measured by the **explained variance**.\n",
        "\n",
        "This notebook will display the explained variance for your dataset and help you choose the right amount of eigenvectors (\"principal components\").\n",
        "\n",
        "* [Setup and loading the data](#setup)\n",
        "* [Preprocessing of the data](#preprocessing)\n",
        "* [Computation of the PCA](#pca)\n",
        "* [Display of the explained variance](#explained-variance)\n",
        "* [Retaining of the most significant components](#final-pca)\n",
        "* [Visualizing the vectors in the original space](#original-space)\n",
        "* [Applying the projection](#apply)\n",
        "\n",
        "\u003ccenter\u003e\u003cstrong\u003eSelect Cell \u003e Run All to execute the whole analysis\u003c/strong\u003e\u003c/center\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and dataset loading \u003ca id\u003d\"setup\" /\u003e \n",
        "\n",
        "First of all, let\u0027s load the libraries that we\u0027ll use"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "%pylab inline\n",
        "import dataiku                               # Access to Dataiku datasets\n",
        "import pandas as pd, numpy as np             # Data manipulation \n",
        "from sklearn.decomposition import PCA        # The main algorithm\n",
        "from matplotlib import pyplot as plt         # Graphing\n",
        "import seaborn as sns                        # Graphing\n",
        "from collections import defaultdict, Counter # Utils\n",
        "sns.set(style\u003d\"white\")                       # Tuning the style of charts\n",
        "import warnings                              # Disable some warnings\n",
        "warnings.filterwarnings(\"ignore\",category\u003dDeprecationWarning)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first thing we do is now to load the dataset and put aside the three main types of columns:\n",
        "\n",
        "* Numerics\n",
        "* Categorical\n",
        "* Dates\n",
        "\n",
        "Since analyzing PCA requires to have the data in memory, we are only going to load a sample of the data. Modify the following cell to change the size of the sample.\n",
        "\n",
        "Also, by default, date features are not kept. Modify the following cell to change that."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": true
      },
      "source": [
        "dataset_limit \u003d 10000\n",
        "keep_dates \u003d False"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load a DSS dataset as a Pandas dataframe"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# Take a handle on the dataset\n",
        "mydataset \u003d dataiku.Dataset(\"test_prepared_scored\")\n",
        "\n",
        "# Load the first lines.\n",
        "# You can also load random samples, limit yourself to some columns, or only load\n",
        "# data matching some filters.\n",
        "#\n",
        "# Please refer to the Dataiku Python API documentation for more information\n",
        "df \u003d mydataset.get_dataframe(limit \u003d dataset_limit)\n",
        "\n",
        "df_orig \u003d df.copy()\n",
        "\n",
        "# Get the column names\n",
        "numerical_columns \u003d list(df.select_dtypes(include\u003d[np.number]).columns)\n",
        "categorical_columns \u003d list(df.select_dtypes(include\u003d[object]).columns)\n",
        "date_columns \u003d list(df.select_dtypes(include\u003d[\u0027\u003cM8[ns]\u0027]).columns)\n",
        "\n",
        "# Print a quick summary of what we just loaded\n",
        "print \"Loaded dataset\"\n",
        "print \"   Rows: %s\" % df.shape[0]\n",
        "print \"   Columns: %s (%s num, %s cat, %s date)\" % (df.shape[1], \n",
        "                                                    len(numerical_columns), len(categorical_columns),\n",
        "                                                    len(date_columns))"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing of the data \u003ca id\u003d\"preprocessing\" /\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Keep the dates as features if requested by the user"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "columns_to_drop \u003d []\n",
        "\n",
        "if keep_dates:\n",
        "    df[date_columns] \u003d df[date_columns].astype(int)*1e-9\n",
        "else:\n",
        "    columns_to_drop.extend(date_columns)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get rid of the columns that contain too many unique values"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "DROP_LIMIT_ABS \u003d 200\n",
        "CAT_DROP_LIMIT_RATIO \u003d 0.5\n",
        "for feature in categorical_columns:\n",
        "    nu \u003d df[feature].nunique()\n",
        "    \n",
        "    if nu \u003e DROP_LIMIT_ABS or nu \u003e CAT_DROP_LIMIT_RATIO*df.shape[0]:\n",
        "        print \"Dropping feature %s with %s values\" % (feature, nu)\n",
        "        columns_to_drop.append(feature)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then need to impute missing values"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# Use mean for numerical features\n",
        "for feature in numerical_columns:\n",
        "    v \u003d df[feature].mean()\n",
        "    if np.isnan(v):\n",
        "        v \u003d 0\n",
        "    print \"Filling %s with %s\" % (feature, v)\n",
        "    df[feature] \u003d df[feature].fillna(v)\n",
        "    \n",
        "# Use mode for categorical features\n",
        "for feature in categorical_columns:\n",
        "    v \u003d df[feature].value_counts().index[0]\n",
        "    df[feature] \u003d df[feature].fillna(v)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Drop the columns"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "print \"Dropping the following columns: %s\" % columns_to_drop\n",
        "df \u003d df.drop(columns_to_drop, axis\u003d1)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For all categorical features, we are going to \"dummy-encode\" them (also sometimes called one-hot encoding).\n",
        "\n",
        "Basically, a categorical feature is replaced by one column per value. Each created value contains 0 or 1 depending on whether the original value was the one of the column."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# For categorical variables with more than that many values, we only keep the most frequent ones\n",
        "LIMIT_DUMMIES \u003d 100\n",
        "\n",
        "# Only keep the top 100 values\n",
        "def select_dummy_values(train, features):\n",
        "    dummy_values \u003d {}\n",
        "    for feature in features:\n",
        "        values \u003d [\n",
        "            value\n",
        "            for (value, _) in Counter(train[feature]).most_common(LIMIT_DUMMIES)\n",
        "        ]\n",
        "        dummy_values[feature] \u003d values\n",
        "    return dummy_values\n",
        "\n",
        "DUMMY_VALUES \u003d select_dummy_values(df, [x for x in categorical_columns if not x in columns_to_drop])\n",
        "\n",
        "\n",
        "def dummy_encode_dataframe(df):\n",
        "    for (feature, dummy_values) in DUMMY_VALUES.items():\n",
        "        for dummy_value in dummy_values:\n",
        "            dummy_name \u003d u\u0027%s_value_%s\u0027 % (feature, dummy_value.decode(\u0027utf-8\u0027))\n",
        "            df[dummy_name] \u003d (df[feature] \u003d\u003d dummy_value).astype(float)\n",
        "        del df[feature]\n",
        "        print \u0027Dummy-encoded feature %s\u0027 % feature\n",
        "\n",
        "dummy_encode_dataframe(df)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we rescale the whole data"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "X \u003d df.values\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "ss \u003d StandardScaler().fit(X)\n",
        "X_std \u003d ss.transform(X)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Computation of the PCA \u003ca id\u003d\"pca\" /\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\u0027s \"fit\" the PCA algorithm (in other words, let\u0027s compute the singular value decomposition)"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "sklearn_pca \u003d PCA()\n",
        "Y_sklearn \u003d sklearn_pca.fit_transform(X_std)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here the PCA is a full SVD (k\u003dd, we have not yet applied any \"reduction\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Display of the explained variance of the eigenvectors. \u003ca id\u003d\"explained-variance\" /\u003e\n",
        "\n",
        "The first thing to do after fitting a PCA algorihtm is to plot the **explained variance** of each eigenvector (how much information from the original data does each vector contain).\n",
        "\n",
        "We also compute how many of these vectors (in order) must be used to retain 90% of the variance of the original dataset (you can change that figure below)"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": true
      },
      "source": [
        "VARIANCE_TO_KEEP \u003d 0.9"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "plt.bar(range(sklearn_pca.n_components_), sklearn_pca.explained_variance_ratio_, alpha\u003d0.5, align\u003d\u0027center\u0027,label\u003d\u0027individual explained variance\u0027)\n",
        "plt.step(range(sklearn_pca.n_components_), [sklearn_pca.explained_variance_ratio_[:y].sum() for y in range(1,sklearn_pca.n_components_+1)], alpha\u003d0.5, where\u003d\u0027mid\u0027,label\u003d\u0027cumulative explained variance\u0027)\n",
        "plt.axhline(y\u003d0.95, linewidth\u003d2, color \u003d \u0027r\u0027)\n",
        "plt.ylabel(\u0027Explained variance ratio\u0027)\n",
        "plt.xlabel(\u0027Principal components\u0027)\n",
        "plt.xlim([0, sklearn_pca.n_components_])\n",
        "plt.legend(loc\u003d\u0027best\u0027)\n",
        "plt.tight_layout()\n",
        "\n",
        "keep_recommend \u003d [sklearn_pca.explained_variance_ratio_[:y].sum()\u003eVARIANCE_TO_KEEP for y in range(1,sklearn_pca.n_components_+1)].count(False)\n",
        "print \"Number of components to keep to retain %s%% of the variance:\" % (100*VARIANCE_TO_KEEP), keep_recommend, \"out of the original\", sklearn_pca.n_components_"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Retaining only some vectors \u003ca id\u003d\"final-pca\" /\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You should decide now how many components you want to keep and change the following parameter.\n",
        "\n",
        "By default we keep the recommended value from the above figure"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": true
      },
      "source": [
        "retained_components_number \u003d keep_recommend"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\u0027s run the PCA again but with a limited number of components this time"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "sklearn_pca_final \u003d PCA(n_components\u003dretained_components_number)\n",
        "Y_sklearn_final \u003d sklearn_pca_final.fit_transform(X_std)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing the eigenvectors in the original feature space \u003ca id\u003d\"original-space\" /\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Decomposition heatmap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each of our eigenvectors has a linear decomposition in the original feature space.\n",
        "\n",
        "To understand which features were the most important, we can see how our eigenvectors are made of each original feature."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# For display reasons, we don\u0027t show all components if more than 50 (same for input variables)\n",
        "n_components_to_show \u003d min(50, sklearn_pca_final.n_components_)\n",
        "n_input_features \u003d sklearn_pca_final.components_.shape[1]\n",
        "\n",
        "decomp_df \u003d pd.DataFrame(sklearn_pca_final.components_[0:n_components_to_show],\n",
        "                            columns\u003ddf.columns)\n",
        "if decomp_df.shape[1] \u003e 50:\n",
        "    decomp_df \u003d decomp_df[decomp_df.columns[0:50]]\n",
        "\n",
        "fig \u003d plt.figure(figsize\u003d(n_input_features, n_components_to_show))\n",
        "sns.set(font_scale\u003d3)\n",
        "sns.heatmap(decomp_df, square\u003dTrue)\n",
        "sns.set(font_scale\u003d1)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualizing projected vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The final visualization that we can build is the visualization of both the original dataset and the transformed dataset,\n",
        "in the original feature space.\n",
        "\n",
        "We are going to select two features of the original dataset, and show on a XY graph:\n",
        "\n",
        "* A scatterplot of the original dataset\n",
        "* A scatterplot of the reduced dataset (after losing the unexplained varaince)\n",
        "* The projection of the first two eigenvectors on the two selected features."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "if len(numerical_columns) \u003e\u003d 2:\n",
        "    feat1 \u003d numerical_columns[0]\n",
        "    feat2 \u003d numerical_columns[1]\n",
        "else:\n",
        "    raise ValueError(\"Failed to automatically select proper variables to plot, please select manually\")\n",
        "    \n",
        "print \"Will plot on these two features: \u0027%s\u0027 and \u0027%s\u0027\" % (feat1, feat2)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": true
      },
      "source": [
        "# Uncomment this to take control on the two variables\n",
        "# feat1 \u003d \"my_feat1\"\n",
        "# feat2 \u003d \"my_feat2\""
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "idx_feat_1 \u003d list(df.columns).index(feat1)\n",
        "idx_feat_2 \u003d list(df.columns).index(feat2)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "samp\u003d1000\n",
        "X_std_samp \u003d np.random.choice(X_std.shape[0], samp)\n",
        "plt.plot(X_std[X_std_samp, idx_feat_1], X_std[X_std_samp , idx_feat_2], \u0027o\u0027, alpha\u003d0.1)\n",
        "colors \u003d [\"green\", \"red\"]\n",
        "i \u003d 0\n",
        "for length, vector in zip(sklearn_pca_final.explained_variance_ratio_, sklearn_pca_final.components_)[0:2]:\n",
        "    i \u003d i+1\n",
        "    i \u003d i % len(colors)\n",
        "    v \u003d vector * 50 * length\n",
        "    plt.plot([0, v[idx_feat_1]], [0, v[idx_feat_2]], \u0027-k\u0027, lw\u003d3, color\u003dcolors[i], label\u003d\u0027PCA eigenvector \u0027 + str(i))\n",
        "plt.xlabel(feat1)\n",
        "plt.ylabel(feat2)\n",
        "plt.title(\u0027Projection of the first two eigenvectors of the PCA on the rescaled space (\u0027 + feat1 + \u0027 / \u0027 + feat2 + \u0027)\u0027)\n",
        "plt.legend(loc\u003d\u0027upper right\u0027)\n",
        "plt.axis(\"equal\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "plt.plot(X[X_std_samp, idx_feat_1], X[X_std_samp , idx_feat_2], \u0027o\u0027, alpha\u003d0.1)\n",
        "colors \u003d [\"green\", \"red\"]\n",
        "i \u003d 0\n",
        "for length, vector in zip(sklearn_pca_final.explained_variance_ratio_, sklearn_pca_final.components_)[0:2]:\n",
        "    i \u003d i+1\n",
        "    i \u003d i % len(colors)\n",
        "    #print vector\n",
        "    v \u003d ss.inverse_transform(vector * length * 50)\n",
        "    #print v\n",
        "    plt.plot([ss.mean_[idx_feat_1], v[idx_feat_1]], [ss.mean_[idx_feat_2], v[idx_feat_2]], \u0027-k\u0027, lw\u003d3, color\u003dcolors[i], label\u003d\u0027PCA eigenvector \u0027 + str(i))\n",
        "plt.xlabel(feat1)\n",
        "plt.ylabel(feat2)\n",
        "plt.title(\u0027Projection of the first two eigenvectors of the PCA on the original space (\u0027 + feat1 + \u0027 / \u0027 + feat2 + \u0027)\u0027)\n",
        "plt.legend(loc\u003d\u0027upper right\u0027)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "samp\u003d1000\n",
        "X_std_samp \u003d np.random.choice(X_std.shape[0], samp)\n",
        "X_new \u003d sklearn_pca_final.inverse_transform(Y_sklearn_final)\n",
        "plt.plot(X_std[X_std_samp, idx_feat_1], X_std[X_std_samp, idx_feat_2], \u0027o\u0027, alpha\u003d0.2, color\u003d\"blue\", label\u003d\"Rescaled original data\")\n",
        "plt.plot(X_new[X_std_samp, idx_feat_1], X_new[X_std_samp, idx_feat_2], \u0027ob\u0027, alpha\u003d0.5, color\u003d\"red\", label\u003d\"Inverse transform after PCA\")\n",
        "plt.xlabel(feat1)\n",
        "plt.ylabel(feat2)\n",
        "plt.title(\u0027Drift of sample values due to the loss of variance after PCA\u0027)\n",
        "plt.legend(loc\u003d\u0027upper right\u0027)\n",
        "plt.axis(\"equal\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Applying the projection \u003ca id\u003d\"apply\" /\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we probably want to actually apply the PCA on the original data, which gives us the projected dataset"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "df_PCA \u003d pd.DataFrame(Y_sklearn_final, columns\u003d[(\"PCA_component_\" + str(comp)) for comp in range(sklearn_pca_final.n_components)])\n",
        "\n",
        "# Inserts back the date columns in the dataFrame with PCA applied\n",
        "for date_col_idx in range(len(date_columns)):\n",
        "    col \u003d date_columns[date_col_idx]\n",
        "    df_PCA.insert(date_col_idx , col, df_orig[col])"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# Uncomment to display the head of the transformed matrix\n",
        "#df_PCA.head()"
      ],
      "outputs": []
    }
  ]
}