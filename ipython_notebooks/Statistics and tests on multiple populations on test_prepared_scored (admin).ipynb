{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.10"
    },
    "analyzedDataset": "test_prepared_scored",
    "creator": "admin",
    "tags": [],
    "customFields": {}
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Statistical analysis and tests (Multiple Populations) on test_prepared_scored"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Univariate analysis is perhaps the simplest form of statistical analysis. The key fact is that only one variable is involved.\n",
        "\n",
        "Bivariate analysis involves the analysis of two variables (often denoted as X, Y), for the purpose of determining the empirical relationship between them.\n",
        "\n",
        "Hypothesis tests are used in determining what outcomes of a study would lead to a rejection of the null hypothesis for a pre-specified level of significance.\n",
        "\n",
        "* [Setup and loading the data](#setup)\n",
        "* [Preprocessing of the data](#preprocessing)\n",
        "* [Statistical analysis and vizualisation](#general)\n",
        "* [Single population tests](#tests_single)\n",
        "* [Two-population tests](#tests_two_pop)\n",
        "\n",
        "\u003ccenter\u003e\u003cstrong\u003eSelect Cell \u003e Run All to execute the whole analysis\u003c/strong\u003e\u003c/center\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and dataset loading \u003ca id\u003d\"setup\" /\u003e \n",
        "\n",
        "First of all, let\u0027s load the libraries that we\u0027ll use"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "%pylab inline"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "import dataiku                               # Access to Dataiku datasets\n",
        "import pandas as pd, numpy as np             # Data manipulation \n",
        "from matplotlib import pyplot as plt         # Graphing\n",
        "import seaborn as sns                        # Graphing\n",
        "#sns.set(style\u003d\"white\")                       # Tuning the style of charts\n",
        "import warnings                              # Disable some warnings\n",
        "warnings.filterwarnings(\"ignore\",category\u003dDeprecationWarning)\n",
        "from scipy import stats                      # Stats"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first thing we do is now to load the dataset and put aside the three main types of columns:\n",
        "\n",
        "* Numerics\n",
        "* Categorical\n",
        "* Dates\n",
        "\n",
        "Statistical analysis requires having the data in memory, we are only going to load a sample of the data. Modify the following cell to change the size of the sample."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": true
      },
      "source": [
        "dataset_limit \u003d 10000"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load a DSS dataset as a Pandas dataframe"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# Take a handle on the dataset\n",
        "mydataset \u003d dataiku.Dataset(\"test_prepared_scored\")\n",
        "\n",
        "# Load the first lines.\n",
        "# You can also load random samples, limit yourself to some columns, or only load\n",
        "# data matching some filters.\n",
        "#\n",
        "# Please refer to the Dataiku Python API documentation for more information\n",
        "df \u003d mydataset.get_dataframe(limit \u003d dataset_limit)\n",
        "\n",
        "# Due to a bug in the current release (0.7) of seaborn we will need to strip non ASCII characters from columns...\n",
        "import unicodedata\n",
        "def strip_accents(s):\n",
        "    return \u0027\u0027.join(c for c in unicodedata.normalize(\u0027NFD\u0027, s) if unicodedata.category(c) !\u003d \u0027Mn\u0027)\n",
        "df.columns \u003d [strip_accents(col) for col in df.columns]\n",
        "\n",
        "df_orig \u003d df.copy()\n",
        "\n",
        "# Get the column names\n",
        "numerical_columns \u003d list(df.select_dtypes(include\u003d[np.number]).columns)\n",
        "categorical_columns \u003d list(df.select_dtypes(include\u003d[object]).columns)\n",
        "date_columns \u003d list(df.select_dtypes(include\u003d[\u0027\u003cM8[ns]\u0027]).columns)\n",
        "\n",
        "# Print a quick summary of what we just loaded\n",
        "print \"Loaded dataset\"\n",
        "print \"   Rows: %s\" % df.shape[0]\n",
        "print \"   Columns: %s (%s num, %s cat, %s date)\" % (df.shape[1], \n",
        "                                                    len(numerical_columns), len(categorical_columns),\n",
        "                                                    len(date_columns))"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing of the data \u003ca id\u003d\"preprocessing\" /\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We assume that the values are in the first numerical column, and population labels in the first categorical column."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "value_col \u003d numerical_columns[0]\n",
        "population_col \u003d categorical_columns[0]"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Uncomment the following lines to take control on this"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "#value_col \u003d u\u0027my_value_column\u0027\n",
        "#population_col \u003d u\u0027my_population_column\u0027\n",
        "print \"Selected value and population columns are \u0027%s\u0027 and \u0027%s\u0027\" % (value_col, population_col)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We impute missing values in the value column"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# Use mean for numerical features\n",
        "v \u003d df[value_col].mean()\n",
        "if np.isnan(v):\n",
        "    v \u003d 0\n",
        "print \"Filling value column \u0027%s\u0027 with %s\" % (value_col, v)\n",
        "df[value_col] \u003d df[value_col].fillna(v)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We get the list of population names from the dataset and plot the count for each value.\n",
        "\n",
        "We also create a dataset containing only values with more than 10 samples, for plotting histograms in the following."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "populations \u003d [ [item, df[df[population_col] \u003d\u003d item][value_col]] for item in df[population_col].value_counts().index]\n",
        "pop_mult_val \u003d df[population_col].value_counts()[df[population_col].value_counts() \u003e 10]\n",
        "df_mult_val \u003d df[[value_col, population_col]][df[population_col].isin(pop_mult_val.index)]"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "plt.figure(figsize\u003d(15, 6))\n",
        "plt.subplot(121)\n",
        "sns.countplot(y\u003dpopulation_col, data\u003ddf.sort_values(population_col))\n",
        "plt.subplot(122)\n",
        "df[population_col].value_counts().plot(kind\u003d\u0027bar\u0027)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Statistical analysis and vizualisation \u003ca id\u003d\"general\" /a\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### General statistics\n",
        "Number of records, mean, standard deviation, minimal value, quartiles, maximum value, mode, variance, skewness and kurtosis."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "stats_list \u003d []\n",
        "cols \u003d [\u0027count\u0027, \u0027mean\u0027, \u0027std\u0027, \u0027min\u0027, \u002725%\u0027, \u002750%\u0027, \u002775%\u0027, \u0027max\u0027, \u0027mode\u0027, \u0027var\u0027, \u0027skew\u0027, \u0027kurtosis\u0027]\n",
        "for pop in populations:\n",
        "    stats_list.append([el for el in pop[1].describe()] + [NaN if pop[1].mode().empty else pop[1].mode()[0],pop[1].var(),pop[1].skew(),pop[1].kurtosis()])\n",
        "stats_df \u003d pd.DataFrame(stats_list, columns\u003dcols, index\u003d[pop[0] for pop in populations])\n",
        "\n",
        "stats_df.head()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can use plots to visualize statistics about your data\n",
        "\n",
        "You can try: \u0027count\u0027, \u0027mean\u0027, \u0027std\u0027, \u0027min\u0027, \u002725%\u0027, \u002750%\u0027, \u002775%\u0027, \u0027max\u0027, \u0027mode\u0027, \u0027var\u0027, \u0027skew\u0027, \u0027kurtosis\u0027"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "plt.figure(figsize\u003d(18,6))\n",
        "stats_df[\u0027count\u0027].plot(kind\u003d\u0027bar\u0027)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Histogram \u0026 Distplot\n",
        "Histograms let you see the number of occurrences in your value column for each population."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "g \u003d sns.FacetGrid(df[[value_col, population_col]], col\u003dpopulation_col, col_wrap\u003d4)\n",
        "g.map(plt.hist, value_col);"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Distplots combine an histogram with a kernel density estimation. We plot these only for populations with more than 10 occurrences."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "g \u003d sns.FacetGrid(df_mult_val, col\u003dpopulation_col, col_wrap\u003d4)\n",
        "g.map(sns.distplot, value_col);"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Histograms and distplots for all populations can also be displayed on the same graph. Hard to read if you have many populations."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "plt.figure(figsize\u003d(15,10))\n",
        "plt.title(\"Histograms of all populations\")\n",
        "for pop in pop_mult_val.index:\n",
        "    plt.hist(df_mult_val[df_mult_val[population_col]\u003d\u003dpop][value_col], label \u003d  pop)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "plt.figure(figsize\u003d(15,10))\n",
        "plt.title(\"Distplots of all populations\")\n",
        "for pop in pop_mult_val.index:\n",
        "    sns.distplot(df_mult_val[df_mult_val[population_col]\u003d\u003dpop][value_col], kde_kws\u003d{\"label\": pop})\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Box plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A simple way of representing statistical data on a plot in which a rectangle is drawn to represent the second and third quartiles, with a vertical line inside to indicate the median value. The lower and upper quartiles are shown as horizontal lines either side of the rectangle. Plotted only for populations with more than 10 elements."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "plt.figure(figsize\u003d(15,10))\n",
        "sns.boxplot(x\u003dvalue_col, y\u003dpopulation_col, data\u003ddf_mult_val);"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Violin plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The violin plot is similar to box plots, except that they also show the probability density of the data at different values. Violin plots include a marker for the median of the data and a box indicating the interquartile range, as in standard box plots. Overlaid on this box plot is a kernel density estimation. "
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "plt.figure(figsize\u003d(15,10))\n",
        "sns.violinplot(x\u003dvalue_col, y\u003dpopulation_col, data\u003ddf_mult_val);"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Letter value plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Letter value plots are an improvement upon boxplots for large datasets.\n",
        "\n",
        "They display the median and the quartiles, like a standard box plot, but will also draw boxes for subsequent \"eights\", \"sixteenth\" etc... which are generically called letter values.\n",
        "\n",
        "A cut off condition will leave a reasonable number of outliers out of the final boxes, helping you spot them easily.\n",
        "\n",
        "Letter valuer plot give a good sense of the distribution of data, and of its skewness.\n",
        "\n",
        "Plotted only for populations with more than 10 elements."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "plt.figure(figsize\u003d(15,10))\n",
        "sns.lvplot(x\u003dvalue_col, y\u003dpopulation_col, data\u003ddf_mult_val);"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Statistical testing \u003ca id\u003d\"tests\" /a\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Statistical tests will be computed by default for the two largest populations found."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "[[pop_name_1, df_pop_1], [pop_name_2, df_pop_2]] \u003d [ pop for pop in populations[0:2]]\n",
        "print \"Series \u0027%s\u0027 has %s and series \u0027%s\u0027 has %s records\" % (pop_name_1, df_pop_1.count(), pop_name_2, df_pop_2.count())"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Reminder:** For a given significance level (e.g. 0.05), if the resulting p-value is smaller (p \u003c 0.05), the null hypothesis is rejected. Otherwise (p â‰¥ 0.05) it cannot be rejected."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": true
      },
      "source": [
        "# Define your confidence threshold here, default is 0.05\n",
        "confidence \u003d 0.05"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": true
      },
      "source": [
        "def analyse_results(confidence, pvalue, message, population_name):\n",
        "    if pvalue \u003c confidence:\n",
        "        print \"The hypothesis of \" + message + \" for \"+ population_name + \" is rejected with pvalue %s (smaller than %s)\" % (pvalue, confidence)\n",
        "    else:\n",
        "        print \"The hypothesis of \" + message + \" for \"+ population_name + \" can not be rejected, pvalue was %s (greater than %s)\" % (pvalue, confidence)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Single population tests \u003ca id\u003d\"tests_single\" /a\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Goodness of fit with a normal law: Shapiro-Wilk test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The null-hypothesis of this test is that the population is normally distributed."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "pvalue_1 \u003d stats.shapiro(df_pop_1)[1]\n",
        "pvalue_2 \u003d stats.shapiro(df_pop_2)[1]\n",
        "test \u003d \u0027normal distribution\u0027\n",
        "analyse_results(confidence, pvalue_1, test, pop_name_1)\n",
        "analyse_results(confidence, pvalue_2, test, pop_name_2)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test for the average value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The null-hypothesis of this test is that the population has the specified mean."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": true
      },
      "source": [
        "# Define the mean you ant to test for here\n",
        "tested_mean \u003d 0"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "pvalue_1 \u003d stats.ttest_1samp(df_pop_1, tested_mean).pvalue\n",
        "pvalue_2 \u003d stats.ttest_1samp(df_pop_2, tested_mean).pvalue\n",
        "test \u003d \u0027mean\u003d%s\u0027 % (tested_mean)\n",
        "analyse_results(confidence, pvalue_1, test, pop_name_1)\n",
        "analyse_results(confidence, pvalue_2, test, pop_name_2)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Two - population tests \u003ca id\u003d\"tests_two_pop\" /a\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Student test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The null-hypothesis of this test is that both populations have the same average, variance is assumed to be equal."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "pvalue \u003d stats.ttest_ind(df_pop_1, df_pop_2).pvalue\n",
        "test \u003d \u0027equal averages\u0027\n",
        "analyse_results(confidence, pvalue, test, pop_name_1 + \" and \" + pop_name_2)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Kolmogorov-Smirnov test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The null-hypothesis of this test is that both populations follow the same distribution."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "pvalue \u003d stats.ks_2samp(df_pop_1, df_pop_2).pvalue\n",
        "test \u003d \u0027same distributions\u0027\n",
        "analyse_results(confidence, pvalue, test, pop_name_1 + \" and \" + pop_name_2)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Other tests**\n",
        "\n",
        "You can use the Kruskal-Wallis H-test to test for **equal median** with `stats.kruskal`\n",
        "\n",
        "You can use the Levene test to test for **equal variance** with `stats.levene`"
      ]
    }
  ]
}