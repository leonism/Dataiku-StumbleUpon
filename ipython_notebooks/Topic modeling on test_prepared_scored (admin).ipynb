{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.9"
    },
    "analyzedDataset": "test_prepared_scored",
    "creator": "admin",
    "tags": [],
    "customFields": {}
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Topic Modeling on test_prepared_scored\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Topic models are statistical models that aim to discover the \u0027hidden\u0027 thematic structure in a collection of documents, i.e. identify possible topics in our corpus. It is an interative process by nature, as it is crucial to determine the right number of topics. \n",
        "\n",
        "This notebook is organised as follows:\n",
        "\n",
        "* [Setup and dataset loading](#setup)\n",
        "* [Text Processing:](#text_process) Before feeding the data to a machine learning model, we need to convert it into numerical features.\n",
        "* [Topics Extraction Models:](#mod) We present two differents models from the sklearn library: NMF and LDA.\n",
        "* [Topics Visualisation with pyLDAvis](#viz)\n",
        "* [Topics Clustering:](#clust)  We try to understand how topics relate to each other.\n",
        "* [Further steps](#next)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and dataset loading \u003ca id\u003d\"setup\" /\u003e \n",
        "\n",
        "First of all, let\u0027s load the libraries that we\u0027ll use.\n",
        "\n",
        "**This notebook requires the installation of the [pyLDAvis](https://pyldavis.readthedocs.io/en/latest/readme.html#installation) package.**\n",
        "[See here for help with intalling python packages.](https://www.dataiku.com/learn/guide/code/python/install-python-packages.html)"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "%pylab inline\n",
        "import warnings                         # Disable some warnings\n",
        "warnings.filterwarnings(\"ignore\",category\u003dDeprecationWarning)\n",
        "import dataiku\n",
        "from dataiku import pandasutils as pdu\n",
        "import pandas as pd,  seaborn as sns\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.feature_extraction import text \n",
        "\n",
        "from sklearn.decomposition import LatentDirichletAllocation,NMF\n",
        "import pyLDAvis.sklearn\n",
        "pyLDAvis.enable_notebook()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "dataset_limit \u003d 10000\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first thing we do is now to load the dataset and identify possible text columns."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": false
      },
      "source": [
        "# Take a handle on the dataset\n",
        "mydataset \u003d dataiku.Dataset(\"test_prepared_scored\")\n",
        "\n",
        "# Load the first lines.\n",
        "# You can also load random samples, limit yourself to some columns, or only load\n",
        "# data matching some filters.\n",
        "#\n",
        "# Please refer to the Dataiku Python API documentation for more information\n",
        "df \u003d mydataset.get_dataframe(limit \u003d dataset_limit)\n",
        "\n",
        "df_orig \u003d df.copy()\n",
        "\n",
        "# Get the column names\n",
        "numerical_columns \u003d list(df.select_dtypes(include\u003d[np.number]).columns)\n",
        "categorical_columns \u003d list(df.select_dtypes(include\u003d[object]).columns)\n",
        "date_columns \u003d list(df.select_dtypes(include\u003d[\u0027\u003cM8[ns]\u0027]).columns)\n",
        "\n",
        "# Print a quick summary of what we just loaded\n",
        "print \"Loaded dataset\"\n",
        "print \"   Rows: %s\" % df.shape[0]\n",
        "print \"   Columns: %s (%s num, %s cat, %s date)\" % (df.shape[1], \n",
        "                                                    len(numerical_columns), len(categorical_columns),\n",
        "                                                    len(date_columns))"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By default, we suppose that the text of interest for which we want to extract topics is the first of the categorical columns."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "raw_text_col \u003d categorical_columns[0]\n",
        "\n",
        "# Uncomment this if you want to take manual control over which variables is the text of interest\n",
        "#print df.columns\n",
        "#raw_text_col \u003d \"text_normalized\"\n",
        "\n",
        "raw_text \u003d df[raw_text_col]",
        "\n",
        "# Issue a warning if data contains NaNs\n",
        "if(raw_text.isnull().any()):\n",
        "    print(\u0027\\x1b[33mWARNING: Your text contains NaNs\\x1b[0m\u0027)\n",
        "    print(\u0027Please take care of them, the countVextorizer will not be able to fit your data if it contains empty values.\u0027)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**To test this notebook  on example data uncomment the following cell.**\n",
        "\n",
        "You can test this notebook on the 20newsgroups dataset:"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "#Example on the 20newsgroups\n",
        "#from sklearn.datasets import fetch_20newsgroups\n",
        "#dataset \u003d fetch_20newsgroups(shuffle\u003dTrue, random_state\u003d1,remove\u003d(\u0027headers\u0027, \u0027footers\u0027, \u0027quotes\u0027))\n",
        "#raw_text \u003d dataset.data"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text Processing \u003ca id\u003d\"text_process\" /\u003e \n",
        "\n",
        "We cannot directly feed the text to the Topics Extraction Algorithms. We first need to process the text in order to get numerical vectors. We achieve this by applying either a CountVectorizer() or a TfidfVectorizer(). For more information on those technics, please refer to thid [sklearn documentation](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html).   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As with any text mining task, we first need to remove stop words that provide no useful information about topics. *sklearn* provides a default stop words list for english, but we can alway add to it any custom stop words : \u003ca id\u003d\"stop_words\" /a\u003e"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "custom_stop_words \u003d []\n",
        "#custom_stop_words \u003d [u\u0027did\u0027, u\u0027good\u0027, u\u0027right\u0027, u\u0027said\u0027, u\u0027does\u0027, u\u0027way\u0027,u\u0027edu\u0027, u\u0027com\u0027, u\u0027mail\u0027, u\u0027thanks\u0027, u\u0027post\u0027, u\u0027address\u0027, u\u0027university\u0027, u\u0027email\u0027, u\u0027soon\u0027, u\u0027article\u0027,u\u0027people\u0027, u\u0027god\u0027, u\u0027don\u0027, u\u0027think\u0027, u\u0027just\u0027, u\u0027like\u0027, u\u0027know\u0027, u\u0027time\u0027, u\u0027believe\u0027, u\u0027say\u0027,u\u0027don\u0027, u\u0027just\u0027, u\u0027think\u0027, u\u0027probably\u0027, u\u0027use\u0027, u\u0027like\u0027, u\u0027look\u0027, u\u0027stuff\u0027, u\u0027really\u0027, u\u0027make\u0027, u\u0027isn\u0027]\n",
        "\n",
        "stop_words \u003d text.ENGLISH_STOP_WORDS.union(custom_stop_words)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CountVectorizer() on the text data \u003ca id\u003d\"tfidf\" /\u003e \n",
        "\n",
        "We first initialise a CountVectorizer() object and then apply the fit_transform method to the text.\n"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "cnt_vectorizer \u003d CountVectorizer(strip_accents \u003d \u0027unicode\u0027,stop_words \u003d stop_words,lowercase \u003d True,\n",
        "                                token_pattern \u003d r\u0027\\b[a-zA-Z]{3,}\\b\u0027, max_df \u003d 0.85, min_df \u003d 2)\n",
        "\n",
        "text_cnt \u003d cnt_vectorizer.fit_transform(raw_text)\n",
        "\n",
        "print(text_cnt.shape)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TfidfVectorizer() on the text data \u003ca id\u003d\"tfidf\" /\u003e \n",
        "\n",
        "We first initialise a TfidfVectorizer() object and then apply the fit_transform method to the text."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "tfidf_vectorizer \u003d TfidfVectorizer(strip_accents \u003d \u0027unicode\u0027,stop_words \u003d stop_words,lowercase \u003d True,\n",
        "                                token_pattern \u003d r\u0027\\b[a-zA-Z]{3,}\\b\u0027, max_df \u003d 0.75, min_df \u003d 0.02)\n",
        "\n",
        "text_tfidf \u003d tfidf_vectorizer.fit_transform(raw_text)\n",
        "\n",
        "print(text_tfidf.shape)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the following, we will apply the topics extraction to `text_tidf`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Topics Extraction Models \u003ca id\u003d\"mod\" /\u003e \n",
        "\n",
        "There are two very popular models for topic modelling, both available in the sklearn library: \n",
        "\n",
        "* [NMF (Non-negative Matrix Factorization)](https://en.wikipedia.org/wiki/Non-negative_matrix_factorization),\n",
        "* [LDA (Latent Dirichlet Allocation)](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)\n",
        "\n",
        "Those two topic modeling algorithms infer topics from a collection of texts by viewing each document as a mixture of various topics. The only parameter we need to choose is the number of desired topics `n_topics`.  \n",
        "It is recommended to try different values for `n_topics` in order to find the most insightful topics. For that, we will show below different analyses (most frequent words per topics and heatmaps)."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "n_topics\u003d 10"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use this line for LDA"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "topics_model \u003d LatentDirichletAllocation(n_topics, random_state\u003d0)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Uncomment the following line to try NMF instead."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "#topics_model \u003d NMF(n_topics, random_state\u003d0)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "topics_model.fit(text_tfidf)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Most Frequent Words per Topics\n",
        "An important way to assess the validity of our topic modelling is to directly look at the most frequent words in each topics."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "n_top_words \u003d 10\n",
        "feature_names \u003d tfidf_vectorizer.get_feature_names()\n",
        "\n",
        "def get_top_words_topic(topic_idx):\n",
        "    topic \u003d topics_model.components_[topic_idx]\n",
        "   \n",
        "    print( [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]] )\n",
        "    \n",
        "for topic_idx, topic in enumerate(topics_model.components_):\n",
        "    print (\"Topic #%d:\" % topic_idx )\n",
        "    get_top_words_topic(topic_idx)\n",
        "    print (\"\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pay attention to the words present, if some are very common you may want to go back to the [definition of custom stop words](#stop_words)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Naming the topics\n",
        "\n",
        "Thanks to the above analysis, we can try to name each topics:"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "dict_topic_name \u003d {i: \"topic_\"+str(i) for i in xrange(n_topics)}\n",
        "#dict_topic_name \u003d my_dict_topic_name #Define here your own name mapping and uncomment this !"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the 20newsgroup dataset, if you used the [suggested custom stop words](#stop_words) we suggest these 10 topics"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "#dict_topic_name \u003d {0: \"Posting\", 1: \"Driving\", 2: \"OS (Windows)\", 3: \"Past\", 4: \"Games\", 5: \"Sales\", 6: \"Misc\", 7: \"Christianity\", 8: \"Personal information\", 9: \"Government/Justice\"}"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Topics Heatmaps\n",
        "\n",
        "Another visual helper to better understand the found topics is to look at the heatmap for the document-topic and topic-words matrices. This gives us the distribution of topics over the collection of documents and the distribution of words over the topics.  \n",
        "We start with the topic-word heatmap where the darker the color is the more the word is representative of the topic:"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "word_model \u003d pd.DataFrame(topics_model.components_.T)\n",
        "word_model.index \u003d feature_names\n",
        "word_model.columns.name \u003d \u0027topic\u0027\n",
        "word_model[\u0027norm\u0027] \u003d (word_model).apply(lambda x: x.abs().max(),axis\u003d1)\n",
        "word_model \u003d word_model.sort_values(by\u003d\u0027norm\u0027,ascending\u003d0) # sort the matrix by the norm of row vector\n",
        "word_model.rename(columns \u003d dict_topic_name, inplace \u003d True) #naming topic\n",
        " \n",
        "del word_model[\u0027norm\u0027]\n",
        "\n",
        "plt.figure(figsize\u003d(9,8))\n",
        "sns.heatmap(word_model[:10]) "
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now display the document-topic heatmap:"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# retrieve the document-topic matrix\n",
        "document_model \u003d pd.DataFrame(topics_model.transform(text_tfidf))\n",
        "document_model.columns.name \u003d \u0027topic\u0027\n",
        "document_model.rename(columns \u003d dict_topic_name, inplace \u003d True) #naming topics\n",
        "\n",
        "plt.figure(figsize\u003d(9,8))\n",
        "sns.heatmap(document_model.sort_index()[:10]) #we limit here to the first 10 texts"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Topic distribution over the corpus  \n",
        "We can look at how the topics are represented in the collection of documents."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "topics_proportion \u003d document_model.sum()/document_model.sum().sum()\n",
        "topics_proportion.plot(kind \u003d \"bar\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For each topic, we can investigate the documents the most representative for the given topic:"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "def top_documents_topics(topic_name, n_doc \u003d 3, excerpt \u003d True):\n",
        "    \u0027\u0027\u0027This returns the n_doc documents most representative of topic_name\u0027\u0027\u0027\n",
        "    \n",
        "    document_index \u003d list(document_model[topic_name].sort_values(ascending \u003d False).index)[:n_doc]\n",
        "    for order, i in enumerate(document_index):\n",
        "        print \"Text for the {}-th most representative document for topic {}:\\n\".format(order + 1,topic_name)\n",
        "        if excerpt:\n",
        "            print raw_text[i][:1000]\n",
        "        else:\n",
        "            print raw_text[i]\n",
        "        print \"\\n******\\n\""
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the 20newsgroup dataset, you can try this to get excerpts from the 3 most representative texts related to the Driving topic"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": false
      },
      "source": [
        "top_documents_topics(\"topic_0\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Topics Visualization with pyLDAvis \u003ca id\u003d\"viz\"\u003e\n",
        "\n",
        "Thanks to the pyLDAvis package, we can easily visualise and interpret the topics that has been fit to our corpus of text data."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "pyLDAvis.sklearn.prepare(topics_model, text_tfidf, tfidf_vectorizer)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Topics Clustering  \u003ca id\u003d\"clust\"\u003e  \n",
        "\n",
        "Once we have fitted topics on the text data, we can try to understand how they relate to one another: we achieve this by doing a hierachical clustering on the topics. We propose two methods, the first is based on a correlation table between topics, the second on a contigency table."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# correlation matrix between topics\n",
        "cor_matrix \u003d np.corrcoef(document_model.iloc[:,:n_topics].values,rowvar\u003d0)\n",
        "\n",
        "#Renaming of the index and columns\n",
        "cor_matrix \u003d pd.DataFrame(cor_matrix)\n",
        "cor_matrix.rename(index \u003d dict_topic_name, inplace \u003d True)\n",
        "cor_matrix.rename(columns\u003d dict_topic_name, inplace \u003d True)\n",
        "\n",
        "sns.clustermap(cor_matrix, cmap\u003d\"bone\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# contingency table on the binarized document-topic matrix\n",
        "document_bin_topic \u003d (document_model.iloc[:,:n_topics] \u003e 0.25).astype(int)\n",
        "contingency_matrix \u003d np.dot(document_bin_topic.T.values, document_bin_topic.values )\n",
        "\n",
        "#Renaming of the index and columns\n",
        "contingency_matrix \u003d pd.DataFrame(contingency_matrix)\n",
        "contingency_matrix.rename(index \u003d dict_topic_name, inplace \u003d True)\n",
        "contingency_matrix.rename(columns\u003d dict_topic_name, inplace \u003d True)\n",
        "\n",
        "sns.clustermap(contingency_matrix)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "## Further steps  \u003ca id\u003d\"next\"\u003e  \n",
        "\n",
        "Topics extraction is a vast subject and a notebook can only show so much. There still much thing we could do, here are some ideas:  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1. Discard documents from noise topics\n",
        "The following helper function takes as argument the topics for which we wish to discard documents."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "def remove_doc(*topic_name):\n",
        "    \n",
        "    doc_max_topic \u003d document_model.idxmax(axis \u003d 1)\n",
        "    print \"Removing documents whose main topic are in \", topic_name\n",
        "    doc_max_topic_filtered \u003d doc_max_topic[~doc_max_topic.isin(topic_name)]\n",
        "    return [raw_text[i] for i in doc_max_topic_filtered.index.tolist()]\n",
        "\n",
        "#E.g.: to remove documents whose main topic are topic_1 or topic_3, we would simply call remove_doc(\"topic_0\",\"topic_2\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the 20newsgroup dataset, try this to remove text of topic \"Misc\""
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "#raw_text_filtered \u003d remove_doc(\"Misc\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2. Scoring the topic model on new text\n",
        "Finally, we can score new text with our topic model as follows."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "new_text \u003d raw_text[:3] #Change this to the new text you\u0027d like to score !\n",
        "\n",
        "tfidf_new_text \u003d tfidf_vectorizer.transform(new_text)\n",
        "result \u003d pd.DataFrame(topics_model.transform(tfidf_new_text), columns \u003d [dict_topic_name[i] for i in xrange(n_topics)])\n",
        "sns.heatmap(result)"
      ],
      "outputs": []
    }
  ]
}